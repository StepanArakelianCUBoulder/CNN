{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Histopathologic Cancer Detection Using Deep Learning</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Project Overview</h2>\n",
    "<h5>In this project, we apply deep learning techniques to automatically detect cancer in histopathologic scan images. Our primary goal is to classify images into two categories: those containing tumor tissue and those without, using state-of-the-art image processing and machine learning methods.</h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objectives</h3>\n",
    "\n",
    "***Automated Classification:*** Develop an algorithm to distinguish between cancerous and non-cancerous histopathologic images.\n",
    "\n",
    "***Feature Extraction with Deep Learning:*** Leverage deep neural networks for efficient and accurate feature extraction from histopathologic images.\n",
    "\n",
    "***Model Interpretability:*** Implement techniques to interpret the model's predictions, providing insights into areas of the images most influential in determining the presence of cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Technologies and Libraries</h3>\n",
    "\n",
    "Programming Language: Python <br>\n",
    "Data Handling: NumPy, pandas <br>\n",
    "Image Processing: OpenCV, PIL <br>\n",
    "Data Visualization: matplotlib, seaborn <br>\n",
    "Machine Learning and Deep Learning: scikit-learn, PyTorch <br>\n",
    "Model Interpretation: captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting captum\n",
      "  Obtaining dependency information for captum from https://files.pythonhosted.org/packages/e1/76/b21bfd2c35cab2e9a4b68b1977f7488c246c8cffa31e3361ee7610e8b5af/captum-0.7.0-py3-none-any.whl.metadata\n",
      "  Using cached captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from captum) (3.7.2)\n",
      "Requirement already satisfied: numpy in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from captum) (1.24.4)\n",
      "Requirement already satisfied: torch>=1.6 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from captum) (2.0.1+cu117)\n",
      "Requirement already satisfied: tqdm in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from captum) (4.65.0)\n",
      "Requirement already satisfied: filelock in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from torch>=1.6->captum) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from torch>=1.6->captum) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from torch>=1.6->captum) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from torch>=1.6->captum) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from torch>=1.6->captum) (3.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (4.42.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (10.0.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: colorama in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from tqdm->captum) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from jinja2->torch>=1.6->captum) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages (from sympy->torch>=1.6->captum) (1.2.1)\n",
      "Using cached captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "Installing collected packages: captum\n",
      "Successfully installed captum-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import random\n",
    "from numpy import load\n",
    "from random import shuffle\n",
    "from math import *\n",
    "import os\n",
    "\n",
    "#Visualization :\n",
    "import matplotlib.pyplot as plt\n",
    "# %pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "#Scikit-learn utils, metrics :\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#Sklearn classification :\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Sklearn clustering :\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Dimension reduction :\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Working with images :\n",
    "# %pip install cv2\n",
    "# import cv2\n",
    "from PIL import Image\n",
    "\n",
    "#Model interpretability\n",
    "%pip install captum\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from captum.attr import IntegratedGradients\n",
    "from captum.attr import GradientShap\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import NoiseTunnel\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "#Pytorch specific :\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "# Notebook specific :\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analyzing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dataset Description</h3>\n",
    "\n",
    "The dataset comprises high-resolution histopathologic scan images labeled into two distinct categories:\n",
    "\n",
    "Class 0 (Non-Cancerous): Images showing no signs of tumor tissue. <br>\n",
    "Class 1 (Cancerous): Images where tumor tissue is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Path to local files :\n",
    "path_root = 'data/'\n",
    "path_df = 'data/train_labels.csv'\n",
    "path_to_train = 'data/train/'\n",
    "path_to_test = 'data/test/'\n",
    "path_submission = 'data/sample_submission.csv'\n",
    "\n",
    "\n",
    "df_submission = pd.read_csv(path_submission)\n",
    "df = pd.read_csv(path_df)\n",
    "df_class0 = df.loc[df['label']==0]\n",
    "df_class1 = df.loc[df['label']==1]\n",
    "id_label_map = {k:v for k,v in zip(df.id.values, df.label.values)}\n",
    "dic_classes2 = {0:'No Cancer', 1:'Cancer'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 - Contains NO tumor tissue\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train/2cfd17c4c7a8f5fa05b6789660bc85ebcc5b22d7.tif'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alien\\Downloads\\cnn\\HistopathologicCancerDetection.ipynb Ячейка 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alien/Downloads/cnn/HistopathologicCancerDetection.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m,\u001b[39m7\u001b[39m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alien/Downloads/cnn/HistopathologicCancerDetection.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m16\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alien/Downloads/cnn/HistopathologicCancerDetection.ipynb#W5sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     image \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39;49mopen(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(path_to_train, df_class0[\u001b[39m'\u001b[39;49m\u001b[39mid\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49msample()\u001b[39m.\u001b[39;49mitem()\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m.tif\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alien/Downloads/cnn/HistopathologicCancerDetection.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     plt\u001b[39m.\u001b[39msubplot(\u001b[39m4\u001b[39m,\u001b[39m4\u001b[39m,i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alien/Downloads/cnn/HistopathologicCancerDetection.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39maxis(\u001b[39m'\u001b[39m\u001b[39moff\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\txt_gen\\oobabooga_windows\\installer_files\\env\\lib\\site-packages\\PIL\\Image.py:3218\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3215\u001b[0m     filename \u001b[39m=\u001b[39m fp\n\u001b[0;32m   3217\u001b[0m \u001b[39mif\u001b[39;00m filename:\n\u001b[1;32m-> 3218\u001b[0m     fp \u001b[39m=\u001b[39m builtins\u001b[39m.\u001b[39;49mopen(filename, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m   3219\u001b[0m     exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   3221\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train/2cfd17c4c7a8f5fa05b6789660bc85ebcc5b22d7.tif'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x700 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Images with NO Tumor tissue in the center region (32x32 pixels)\n",
    "print(\"Class 0 - Contains NO tumor tissue\")\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(16):\n",
    "    image = Image.open(os.path.join(path_to_train, df_class0['id'].sample().item()+'.tif'))\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "## Visualizing class 0 :\n",
    "# Images with at least ONE pixel of Tumor tissue in the center region (32x32 pixels)\n",
    "print(\"\\n\",\"Class 1 - Contains at least ONE tumor tissue\")\n",
    "plt.figure(figsize=(7,7))\n",
    "for i in range(16):\n",
    "    image = Image.open(os.path.join(path_to_train, df_class1['id'].sample().item()+'.tif'))\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some details about a random image :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(os.path.join(path_to_train, df_class1['id'].sample().item()+'.tif'))\n",
    "\n",
    "print(\"Image format:\",image.format)\n",
    "print(\"Image mode:\",image.mode)\n",
    "print('Image size:',image.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Custom Dataset Class (Cancer_detection_Dataset):</h4>\n",
    "\n",
    "Purpose: Custom class for loading, transforming, and preprocessing the histopathologic images for use in a PyTorch model. <br>\n",
    "Features: Image resizing, normalization, and augmentation techniques to improve model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cancer_detection_Dataset(Dataset):\n",
    "    \"\"\"Histopathologic-cancer-detection Dataset.\"\"\"\n",
    "    def __init__(self, root_dir, train_file, transform):\n",
    "        self.root_dir = root_dir\n",
    "        self.train_file_path = train_file\n",
    "        self.df_train = pd.read_csv(self.train_file_path)\n",
    "        self.transform = transform\n",
    "        self.classes = list(self.df_train['label'].unique())\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Return (image, target) after resize and preprocessing.\"\"\"\n",
    "        img = os.path.join(self.root_dir, self.df_train.iloc[idx, 0]+'.tif')\n",
    "        \n",
    "        X = Image.open(img)\n",
    "        y = self.class_to_index(self.df_train.iloc[idx, 1])\n",
    "        X = np.array(X)\n",
    "        X = torch.tensor(X)\n",
    "        X = X.float()\n",
    "        X = X.permute(2, 0, 1)\n",
    "\n",
    "        if self.transform:\n",
    "            X = self.transform(X)\n",
    "            #print(X.shape) #: 96x96x3\n",
    "        \n",
    "        y = torch.tensor([y])\n",
    "        return X, y\n",
    "    \n",
    "    def class_to_index(self, class_name):\n",
    "        \"\"\"Returns the index of a given class.\"\"\"\n",
    "        return self.classes.index(class_name)\n",
    "    \n",
    "    def index_to_class(self, class_index):\n",
    "        \"\"\"Returns the class of a given index.\"\"\"\n",
    "        return self.classes[class_index]\n",
    "    \n",
    "    def class_to_label(self, class_name):\n",
    "        \"\"\"Returns the label of a given class\"\"\"\n",
    "        return self.dic_classes[class_name]\n",
    "    \n",
    "    def get_class_count(self):\n",
    "        \"\"\"Return a list of label occurences\"\"\"\n",
    "        cls_count = dict(self.df_train.label.value_counts())\n",
    "        return cls_count\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self.df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations: <br>\n",
    "***Resize:*** Resize the image to a square of size 224 x 224 pixels. <br>\n",
    "***Normalize:*** Normalize the image by subtracting the mean and dividing by the standard deviation of the image dataset. <br>\n",
    "***RandomHorizontalFlip:*** Randomly flip the image horizontally with a probability of 0.5. <br>\n",
    "***RandomVerticalFlip:*** Randomly flip the image vertically with a probability of 0.5. <br>\n",
    "***RandomRotation:*** Randomly rotate the image by a specified angle. <br>\n",
    "***RandomAffine:*** Randomly apply affine transformations to the image. <br>\n",
    "***RandomPerspective:*** Randomly apply perspective transformations to the image. <br>\n",
    "***RandomResizedCrop:*** Randomly crop the image to a specified size and aspect ratio. <br>\n",
    "***ColorJitter:*** Randomly change the brightness, contrast, and saturation of the image. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(90),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.Normalize((179.13/255, 139.29/255, 177.59/255), (46.11/255, 51.17/255, 41.27/255)))\n",
    "\n",
    "transform_test = torch.nn.Sequential(\n",
    "    transforms.CenterCrop(90),\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.Normalize((179.13/255, 139.29/255, 177.59/255), (46.11/255, 51.17/255, 41.27/255)))\n",
    "\n",
    "dataset_train = Cancer_detection_Dataset(root_dir=path_to_train, train_file=path_df, transform=transform_train)\n",
    "dataset_test = Cancer_detection_Dataset(root_dir=path_to_test, train_file=path_submission, transform=transform_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Dataset Split:</h4>\n",
    "\n",
    "***Train:*** 80% <br>\n",
    "***Validation:*** 10% <br>\n",
    "***Test:*** 10% <br>\n",
    "\n",
    "<h4>Dataset Statistics:</h4>\n",
    "\n",
    "***Total Images:*** 220025 <br>\n",
    "***Class 0:*** 130908 <br>\n",
    "***Class 1:*** 89117 <br>\n",
    "***Mean:*** [0.702447, 0.546243, 0.696453] <br>\n",
    "***Standard Deviation:*** [0.238893, 0.282094, 0.216251] <br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Eval_Model():\n",
    "    \"\"\"Train and / or evaluate a choosen model.\"\"\"\n",
    "    def __init__(self, net, dataset_train, dataset_test, batch_size):\n",
    "        self.dataset_train = dataset_train\n",
    "        self.dataset_test = dataset_test\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.validation_split = 0.1\n",
    "        self.shuffle_dataset = True\n",
    "        self.random_seed= 42\n",
    "        \n",
    "        # Average Mean and Std for all images in the training dataset, for all 3 layers\n",
    "        self.mean1 = 179.13/255\n",
    "        self.mean2 = 139.29/255\n",
    "        self.mean3 = 177.59/255\n",
    "        self.std1 = 46.11/255\n",
    "        self.std2 = 51.17/255\n",
    "        self.std3 = 41.27/255\n",
    "        \n",
    "        # Creating data indices for training and validation splits:\n",
    "        dataset_size = len(self.dataset_train)\n",
    "        indices = list(range(dataset_size))\n",
    "        split = int(np.floor(self.validation_split * dataset_size))\n",
    "\n",
    "        if self.shuffle_dataset :\n",
    "            np.random.seed(self.random_seed)\n",
    "            np.random.shuffle(indices)\n",
    "        train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        # Creating data samplers and loaders:\n",
    "        train_sampler = SubsetRandomSampler(train_indices)\n",
    "        valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "        self.train_loader = torch.utils.data.DataLoader(self.dataset_train, self.batch_size, \n",
    "                                                   sampler=train_sampler)\n",
    "        self.validation_loader = torch.utils.data.DataLoader(self.dataset_train, self.batch_size,\n",
    "                                                        sampler=valid_sampler)\n",
    "        self.test_loader = torch.utils.data.DataLoader(self.dataset_test, self.batch_size)\n",
    "        \n",
    "        #Loading the model with cuda or cpu\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.net = net.to(self.device)\n",
    "        print(\"GPU :\",torch.cuda.get_device_name(0), \"activated\")\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Init OK\")\n",
    "        \n",
    "   \n",
    "    def denormalize(self,tensor):\n",
    "        \"\"\"Denormalize tensor to display true image\"\"\"\n",
    "        tensor[0,:,:] = tensor[0,:,:]*self.std1 + self.mean1\n",
    "        tensor[1,:,:] = tensor[1,:,:]*self.std2 + self.mean2\n",
    "        tensor[2,:,:] = tensor[2,:,:]*self.std3 + self.mean3\n",
    "        return tensor\n",
    "    \n",
    "    def auc_score(self, y_pred,y_true):\n",
    "        y_pred = y_pred.cpu().detach()\n",
    "        y_true = y_true.cpu().detach()\n",
    "        score=roc_auc_score(y_true,y_pred)\n",
    "        return score\n",
    "    \n",
    "    \n",
    "    def train(self, epochs:int, lr:float):\n",
    "        torch.cuda.empty_cache()\n",
    "        self.net.to(self.device)\n",
    "        \n",
    "        #Criterion and optimizer :\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(self.net.parameters(), lr=lr)\n",
    "        \n",
    "        #Training :\n",
    "        self.net.train()\n",
    "        liste_loss = []\n",
    "        auc_train_mean, auc_val_mean = [], []\n",
    "        \n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            running_loss = 0.0\n",
    "            i = 0\n",
    "            for data_train, data_val in tqdm(zip(self.train_loader, self.validation_loader)):\n",
    "                # For the training set :\n",
    "                inputs_train, labels_train = data_train\n",
    "                labels_train = labels_train.to(torch.float32)\n",
    "                inputs_train, labels_train = inputs_train.to(self.device), labels_train.to(self.device)\n",
    "\n",
    "                #For the validation set :\n",
    "                inputs_val, labels_val = data_val\n",
    "                labels_val = labels_val.to(torch.float32)\n",
    "                inputs_val, labels_val = inputs_val.to(self.device), labels_val.to(self.device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs_train = self.net(inputs_train)\n",
    "                outputs_train = outputs_train.float()\n",
    "                outputs_train = outputs_train #Torch.round is ok too\n",
    "                outputs_train.to(self.device)\n",
    "\n",
    "                loss = criterion(outputs_train, labels_train)\n",
    "                loss.to(self.device)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # print statistics\n",
    "                running_loss += loss.item()\n",
    "\n",
    "                if i % 20 == 19:    # print every 20 batches\n",
    "                    with torch.no_grad():\n",
    "                        outputs_val = self.net(inputs_val)\n",
    "                        outputs_val = outputs_val.float()\n",
    "                        outputs_val = outputs_val\n",
    "\n",
    "                    auc_train = self.auc_score(outputs_train,labels_train)\n",
    "                    auc_val = self.auc_score(outputs_val,labels_val)\n",
    "                    auc_val_mean.append(auc_val)\n",
    "                    auc_train_mean.append(auc_train)\n",
    "\n",
    "                    print('[%d, %5d] loss: %.3f , AUC train: %.2f , AUC val: %.2f' %\n",
    "                          (epoch + 1, i + 1, running_loss, auc_train, auc_val))\n",
    "                    liste_loss.append(running_loss)\n",
    "                    running_loss = 0.0  \n",
    "                i+=1\n",
    "        print('Finished Training')\n",
    "        print(\"\\n\", \"AUC train mean:\", np.mean(auc_train_mean[-int(len(self.train_loader)/20):]))\n",
    "        print(\"\\n\", \"AUC val mean:\", np.mean(auc_val_mean[-int(len(self.validation_loader)/20):]))\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(liste_loss, label='Training loss')\n",
    "        plt.legend()\n",
    "        plt.title(\"Training loss\")\n",
    "        plt.show()\n",
    "\n",
    "        #Plot AUC evolution for Train and Val :\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(auc_train_mean,label='AUC train')\n",
    "        plt.plot(auc_val_mean,label='AUC val')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def save(self, out_name:str):\n",
    "        torch.save(self.net.state_dict(), out_name)\n",
    "        print('Model weights successfully saved !')\n",
    "        \n",
    "    def load(self, path_load:str):\n",
    "        self.net.load_state_dict(torch.load(path_load))\n",
    "        self.net.to(self.device)\n",
    "        print(\"Model loaded successfully\")\n",
    "        \n",
    "    def show_eval_mosaique(self):\n",
    "        \"\"\" Show a mosaique of 32 images, display\n",
    "        their true labels and the associated prediction\"\"\"\n",
    "        self.net.eval()\n",
    "        liste_auc = []\n",
    "        for i, data in enumerate(self.train_loader):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels  = data\n",
    "                labels = labels.to(torch.float32)\n",
    "                inputs = inputs.to(self.device)\n",
    "\n",
    "                outputs = self.net(inputs)\n",
    "                outputs = torch.round(outputs.float())\n",
    "                liste_auc.append(self.auc_score(outputs.cpu(),labels))\n",
    "            break\n",
    "\n",
    "        #Displaying the images and their true & predicted labels :\n",
    "        plt.figure(figsize=(25,50))\n",
    "        for j, image in enumerate(inputs):\n",
    "            image = image.cpu()\n",
    "            image = self.denormalize(image)\n",
    "            image = np.transpose(image, (1,2,0))\n",
    "            image = np.array(image, dtype=np.uint8)\n",
    "            plt.subplot(16,4,j+1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(f'{dic_classes2[int(labels[j].item())]}, prediction {labels[j].item()==outputs[j].item()}',\n",
    "                     size=13)\n",
    "            plt.axis('off')\n",
    "        print(\"AUC mean on this batch :\", np.mean(liste_auc))\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n', \"Confusion Matrix:\")\n",
    "        conf_matrix = confusion_matrix(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), labels=[0, 1])\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[0, 1])\n",
    "        disp.plot()\n",
    "\n",
    "    \n",
    "    def explain_image(self,image):   \n",
    "        occlusion = Occlusion(self.net)\n",
    "\n",
    "        attributions_occ = occlusion.attribute(image.unsqueeze(0),\n",
    "                                           strides = (3, 4, 4),\n",
    "                                           target=None,\n",
    "                                           sliding_window_shapes=(3,5, 5),\n",
    "                                           baselines=0)\n",
    "\n",
    "        _ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n",
    "                                          np.uint8(np.transpose(self.denormalize(image.squeeze().detach().cpu()), (1,2,0))),\n",
    "                                          [\"original_image\", \"heat_map\"],\n",
    "                                          [\"all\", \"positive\"],\n",
    "                                          show_colorbar=True,\n",
    "                                          outlier_perc=4,\n",
    "                                         cmap='Reds')\n",
    "        \n",
    "    def show_eval_explained(self, explain:bool):\n",
    "        \"\"\" Display a random image in the train set \n",
    "        and its heatmap with the occlusion method\"\"\"\n",
    "        self.net.eval()\n",
    "\n",
    "        rand = random.randint(0,len(self.train_loader))\n",
    "        inputs, labels = self.train_loader.dataset[rand][0], self.train_loader.dataset[rand][1]\n",
    "        labels = labels.to(torch.float32)\n",
    "        inputs_explain = inputs.clone().to(self.device)\n",
    "        inputs = inputs.unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.net(inputs)\n",
    "            print(\"Output probability: %.3f\" % (outputs.item()))\n",
    "            outputs = torch.round(outputs.float())\n",
    "\n",
    "        #Plotting the results :\n",
    "        image = inputs.squeeze().detach().cpu()\n",
    "        image = self.denormalize(image)\n",
    "\n",
    "        image = np.transpose(image, (1,2,0))\n",
    "        image = np.array(image, dtype=np.uint8)\n",
    "\n",
    "        plt.imshow(image)\n",
    "        plt.title(f'{dic_classes2[int(labels.item())]}, prediction {labels.item()==outputs.item()}', size=15)\n",
    "        plt.axis('Off')\n",
    "        plt.show()\n",
    "\n",
    "        if explain :\n",
    "            print(\"Interpreting prediction results. This could take up to 1 long minute...\")\n",
    "            print(\"Highlighting areas that led to that prediction...\")\n",
    "            self.explain_image(inputs_explain)\n",
    "            \n",
    "    def submit_test(self, filename:str):\n",
    "        self.net.eval()\n",
    "        liste_outputs = []\n",
    "        print('Evaluating test set...')\n",
    "        for i, data in tqdm(enumerate(self.test_loader)):\n",
    "            with torch.no_grad():\n",
    "                inputs, labels  = data\n",
    "                labels = labels.to(torch.float32)\n",
    "                inputs = inputs.to(self.device)\n",
    "                outputs = self.net(inputs)\n",
    "                outputs = torch.round(outputs.float())\n",
    "                liste_outputs.append(outputs)\n",
    "        liste_outputs2 = []\n",
    "        for i in tqdm(range(len(liste_outputs))):\n",
    "            for j in liste_outputs[i]:\n",
    "                liste_outputs2.append(int(j.item()))\n",
    "        df_submission['label'] = liste_outputs2\n",
    "        df_submission.to_csv(filename, index=False)\n",
    "        print(\"Submission successfully downloaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Neural Network Architecture</h3>\n",
    "\n",
    "***Model:*** Base: Utilizes DenseNet169 as a pre-trained model for feature extraction. <br>\n",
    "***Input Layer:*** 3-channel RGB image of size 224 x 224 pixels. <br>\n",
    "***Output Layer:*** 2 classes (Cancerous and Non-Cancerous). <br>\n",
    "***Loss Function:*** Cross-Entropy Loss. <br>\n",
    "***Optimizer:*** Adam. <br>\n",
    "***Learning Rate:*** 0.0001. <br>\n",
    "***Batch Size:*** 32. <br>\n",
    "***Epochs:*** 10. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Neural Network :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Net1(nn.Module):\n",
    "    def __init__(self, pretrained):\n",
    "        super(Net1, self).__init__()\n",
    "        self.model = models.densenet169(pretrained=pretrained)\n",
    "        self.linear1 = nn.Linear(1000+2, 512)\n",
    "        self.linear2 = nn.Linear(512, 16)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.elu = nn.ELU()\n",
    "        self.out = nn.Linear(16, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        batch = out.shape[0]\n",
    "        max_pool, _ = torch.max(out, 1, keepdim=True)\n",
    "        avg_pool = torch.mean(out, 1, keepdim=True)\n",
    "\n",
    "        out = out.view(batch, -1)\n",
    "        conc = torch.cat((out, max_pool, avg_pool), 1)\n",
    "\n",
    "        conc = self.linear1(conc)\n",
    "        conc = self.elu(conc)\n",
    "        conc = self.dropout(conc)\n",
    "        conc = self.linear2(conc)\n",
    "        conc = self.elu(conc)\n",
    "\n",
    "        res = self.out(conc)\n",
    "        return torch.sigmoid(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Framework  :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Data Handling: Creating train, validation, and test loaders with appropriate data splits. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = Train_Eval_Model(Net1(pretrained=True), dataset_train, dataset_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Training Process: Details on the training loop, including loss calculation, backpropagation, and optimization. </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.train(epochs=5, lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Model Save/Load: Functions to save and load the trained model for future inference.</h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights successfully saved !\n"
     ]
    }
   ],
   "source": [
    "model.save(out_name='V11_best.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Performance Metrics: Evaluation using metrics such as ROC-AUC scores, loss graphs, and accuracy measurements. </5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.load('V11_best.pth')\n",
    "model.show_eval_mosaique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Model Interpretability with Captum:</h5>\n",
    "<h5> Explanation of Captum's role in highlighting critical regions in the images that contribute to the model's predictions. <br>\n",
    "Visualization of these interpretations to understand the model's focus areas. </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I use the library Captum that estimates which areas of the image are critical for the classifier's decision by occluding them and quantifying how the decision changes.\n",
    "\n",
    "In other words, for images containing a metastase, this script will highlight the part of the image that contains the cancer cell and that led to that prediction !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 1 :\n",
    "model.show_eval_explained(explain=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2 :\n",
    "model.show_eval_explained(explain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Submitting test csv for Kaggle challenge : </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.submit_test(filename='submission_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
